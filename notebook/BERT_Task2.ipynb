{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Task2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6Rj-7pCHz5D",
        "outputId": "738d8898-7165-43d5-8e8f-3974ba8da494"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DllFgq0BA3S-"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaD-nuuzA4lo",
        "outputId": "3687751d-c37c-4a88-a6ea-6043093712a8"
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "task = 'task_2'\n",
        "\n",
        "#2019 datasets also\n",
        "\n",
        "df_test = pd.read_csv(\"hasoc2019_en_test-2919.tsv\",sep='\\t')\n",
        "df_train = pd.read_csv(\"english_dataset.tsv\",sep=\"\\t\")\n",
        "df_train = df_train.dropna()\n",
        "\n",
        "\n",
        "\n",
        "print(len(df_train))\n",
        "print(df_train.head())\n",
        "\n",
        "total_sentences = list(df_train['text'].values)\n",
        "total_labels = list(df_train[task].values)\n",
        "\n",
        "\n",
        "\n",
        "test_sentences = list(df_test['text'].values)\n",
        "test_labels = list(df_test[task].values)\n",
        "\n",
        "def clean_text(sentences):\n",
        "    for index,line in enumerate(sentences):\n",
        "        if \"\\n\" in line:\n",
        "            sentences[index] = line.replace(\"\\n\",\"\")\n",
        "    return sentences\n",
        "        \n",
        "total_sentences = clean_text(total_sentences)\n",
        "test_sentences = clean_text(test_sentences)\n",
        "\n",
        "def clean_labels(labels):\n",
        "    new_list= []\n",
        "    for value in labels:\n",
        "        new_list.append(value.strip())\n",
        "    return new_list\n",
        "\n",
        "total_labels = clean_labels(total_labels)\n",
        "test_labels = clean_labels(test_labels)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(total_labels)\n",
        "encoded_labels = le.transform(total_labels)\n",
        "encoded_test_labels = le.transform(test_labels)\n",
        "print(set(encoded_labels))\n",
        "\n",
        "print(len(total_sentences),len(encoded_labels),len(test_sentences),len(encoded_test_labels))\n",
        "\n",
        "print(df_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5852\n",
            "      text_id                                               text  ... task_2 task_3\n",
            "0  hasoc_en_1  #DhoniKeepsTheGlove | WATCH: Sports Minister K...  ...   NONE   NONE\n",
            "1  hasoc_en_2  @politico No. We should remember very clearly ...  ...   HATE    TIN\n",
            "2  hasoc_en_3  @cricketworldcup Guess who would be the winner...  ...   NONE   NONE\n",
            "3  hasoc_en_4  Corbyn is too politically intellectual for #Bo...  ...   NONE   NONE\n",
            "4  hasoc_en_5  All the best to #TeamIndia for another swimmin...  ...   NONE   NONE\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "{0, 1, 2, 3}\n",
            "5852 5852 1153 1153\n",
            "             text_id  ... task_3\n",
            "0       hasoc_en_902  ...   NONE\n",
            "1       hasoc_en_416  ...   NONE\n",
            "2       hasoc_en_207  ...   NONE\n",
            "3       hasoc_en_595  ...   NONE\n",
            "4       hasoc_en_568  ...    UNT\n",
            "...              ...  ...    ...\n",
            "1148  hasoc_en1_3958  ...   NONE\n",
            "1149  hasoc_en1_4648  ...   NONE\n",
            "1150  hasoc_en1_4832  ...   NONE\n",
            "1151  hasoc_en1_3721  ...   NONE\n",
            "1152   hasoc_en1_991  ...   NONE\n",
            "\n",
            "[1153 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYZGhJ_4C-Dq",
        "outputId": "797d5954-5be2-4221-90cb-2f8f1f0f1c01"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "max_length = 0\n",
        "for sentence in total_sentences:\n",
        "    #print(sentence)\n",
        "    length = len(tokenizer.tokenize(sentence))\n",
        "    if length > max_length:\n",
        "        max_length  = length\n",
        "print(\"max token length is: \",max_length)\n",
        "# max token length obtained is 50\n",
        "# bert tokens are limited to 514 bytes."
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max token length is:  399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VwTBUNhEHzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b8584a-5927-482d-b70d-ea0a3aef015b"
      },
      "source": [
        "def encoder_generator(sentences,labels):\n",
        "    \n",
        "    sent_index = []\n",
        "    input_ids = []\n",
        "    attention_masks =[]\n",
        "\n",
        "    for index,sent in enumerate(sentences):\n",
        "        \n",
        "        sent_index.append(index)\n",
        "        \n",
        "        encoded_dict = tokenizer.encode_plus(sent,\n",
        "                                             add_special_tokens=True,\n",
        "                                             max_length=128,\n",
        "                                             pad_to_max_length=True,\n",
        "                                             truncation = True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_tensors='pt')\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids,dim=0)\n",
        "    attention_masks = torch.cat(attention_masks,dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    sent_index = torch.tensor(sent_index)\n",
        "\n",
        "    return sent_index,input_ids,attention_masks,labels\n",
        "\n",
        "sent_index,input_ids,attention_masks,encoded_label_tensors = encoder_generator(total_sentences,encoded_labels)\n",
        "test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors = encoder_generator(test_sentences,encoded_test_labels)\n",
        "print('Original: ', total_sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  #DhoniKeepsTheGlove | WATCH: Sports Minister Kiren Rijiju issues statement backing MS Dhoni over 'Balidaan Badge', tells BCCI to take up the matter with ICC and keep government in the know as nation's pride is involved    https://t.co/zuo5335Rjr\n",
            "Token IDs: tensor([  101,  1001, 28144, 10698, 20553,  4523, 10760, 23296, 21818,  1064,\n",
            "         3422,  1024,  2998,  2704, 11382,  7389, 15544,  4478,  9103,  3314,\n",
            "         4861,  5150,  5796, 28144, 10698,  2058,  1005, 20222,  2850,  2319,\n",
            "        10780,  1005,  1010,  4136,  4647,  6895,  2000,  2202,  2039,  1996,\n",
            "         3043,  2007, 16461,  1998,  2562,  2231,  1999,  1996,  2113,  2004,\n",
            "         3842,  1005,  1055,  6620,  2003,  2920, 16770,  1024,  1013,  1013,\n",
            "         1056,  1012,  2522,  1013, 16950,  2080, 22275, 19481,  2099,  3501,\n",
            "         2099,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIaQBoXwEhPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d96fb5f-df3a-41d3-a722-916205966989"
      },
      "source": [
        "from torch.utils.data import TensorDataset,random_split\n",
        "\n",
        "dataset = TensorDataset(input_ids,attention_masks,encoded_label_tensors)\n",
        "test_dataset = TensorDataset(test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors)\n",
        "\n",
        "train_size = int(0.75*len(dataset))\n",
        "\n",
        "val_size = len(dataset)-train_size\n",
        "\n",
        "train_dataset,val_dataset = random_split(dataset,[train_size,val_size])\n",
        "\n",
        "print('train data samples is {}'.format(len(train_dataset)))\n",
        "print(\"valid data samples is {}\".format(len(val_dataset)))\n",
        "print(\"test data samples is {}\".format(len(test_dataset)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data samples is 4389\n",
            "valid data samples is 1463\n",
            "test data samples is 1153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8FTVcl6Eowm"
      },
      "source": [
        "from torch.utils.data import DataLoader,RandomSampler,SequentialSampler\n",
        "\n",
        "bs=8\n",
        "\n",
        "train_data_loader = DataLoader(train_dataset,\n",
        "                              sampler=RandomSampler(train_dataset),\n",
        "                              batch_size=bs)\n",
        "valid_data_loader = DataLoader(val_dataset,\n",
        "                              sampler=SequentialSampler(val_dataset),\n",
        "                              batch_size=bs)\n",
        "test_data_loader = DataLoader(test_dataset,\n",
        "                            sampler=SequentialSampler(test_dataset),\n",
        "                            batch_size=bs)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CseI8GoXEwkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d17f34-5df2-4243-99c0-affd48e49515"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
        "                                                     num_labels=len(le.classes_),\n",
        "                                                     output_attentions=False,\n",
        "                                                     output_hidden_states=False,\n",
        "                                                     )\n",
        "#model.cpu()\n",
        "device = \"cuda:0\"\n",
        "model = model.to(device)\n",
        "model.cuda()\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIqD4pu5lsAf"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-8)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs=10\n",
        "total_steps = len(train_data_loader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                           num_warmup_steps=0,\n",
        "                                           num_training_steps=total_steps)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nREL2AQFNZt"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def predictions_labels(preds,labels):\n",
        "    #print(preds.device,labels.device)\n",
        "    pred = torch.argmax(preds,axis=1).flatten()\n",
        "    label = labels.flatten()\n",
        "    return pred,label"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HPQl9hWFSd3"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDFflMIcFVs9"
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    #print(correct.device)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]]).to(device)\n",
        "\n",
        "def predictions_labels(preds,labels):\n",
        "    #print(preds.device,labels.device)\n",
        "    pred = torch.argmax(preds,axis=1).flatten()\n",
        "    label = labels.flatten()\n",
        "    return pred,label"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvi36_J7FZy9"
      },
      "source": [
        "def train():\n",
        "  total_train_loss = 0\n",
        "  total_train_acc = 0\n",
        "    \n",
        "  model.train() # set model in train mode for batchnorm and dropout layers in bert model\n",
        "    \n",
        "  for step,batch in enumerate(train_data_loader):\n",
        "    #print(\"**************************************************************************\")\n",
        "    #print(\"Step : \",step,\"  batch\",len(batch))\n",
        "    #print(\"**************************************************************************\")\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    model.zero_grad()\n",
        "    #loss,logits = model(b_input_ids,attention_mask=b_input_mask,labels=b_labels.long())\n",
        "    outputs = model(b_input_ids,attention_mask=b_input_mask,labels=b_labels.long())\n",
        "    loss = outputs.loss\n",
        "    logits = outputs.logits\n",
        "    #total_train_loss+=loss.detach().numpy()\n",
        "    total_train_loss+=loss.detach()\n",
        "    total_train_acc+=categorical_accuracy(logits,b_labels).item()\n",
        "            \n",
        "    loss.backward()\n",
        "            \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "            \n",
        "    optimizer.step()\n",
        "            \n",
        "    scheduler.step() #go ahead and update the learning rate\n",
        "    #print(total_train_loss,total_train_acc)\n",
        "            \n",
        "  avg_train_loss = total_train_loss/len(train_data_loader)\n",
        "  avg_train_acc = total_train_acc/len(train_data_loader)\n",
        "    \n",
        "  return avg_train_loss,avg_train_acc"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS3kPLQMFgOG"
      },
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "        \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    number_of_eval_steps= 0\n",
        "    \n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    for batch in valid_data_loader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "        #loss, logits = model(b_input_ids,attention_mask= b_input_mask,labels = b_labels.long())\n",
        "          outputs = model(b_input_ids,attention_mask=b_input_mask,labels=b_labels.long())\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        #total_eval_loss+=loss.detach().numpy()\n",
        "\n",
        "        #logits = logits.detach().cpu().numpy()\n",
        "        #label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_loss+=loss.detach()        \n",
        "        logits = logits.detach()\n",
        "        label_ids = b_labels.to(device)\n",
        "\n",
        "        pred,true = predictions_labels(logits,label_ids)\n",
        "        \n",
        "        all_pred_labels.extend(pred.detach().cpu().numpy())\n",
        "        all_true_labels.extend(true.detach().cpu().numpy())\n",
        "    \n",
        "    #print(np.shape(np.array(all_pred_labels).reshape(-1,1)),np.shape(np.array(all_true_labels).reshape(-1,1)))\n",
        "\n",
        "    print(classification_report(all_pred_labels,all_true_labels))\n",
        "    avg_val_accuracy = accuracy_score(all_pred_labels,all_true_labels)\n",
        "    macro_f1_score = f1_score(all_pred_labels,all_true_labels,average='macro')\n",
        "    \n",
        "    avg_val_loss = total_eval_loss/len(valid_data_loader)\n",
        "\n",
        "    print(\"accuracy = {0:.2f}\".format(avg_val_accuracy))\n",
        "    \n",
        "    return avg_val_loss,avg_val_accuracy,macro_f1_score"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLQ5ATlIFoMz"
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmNn6N7wFtj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b158fe-753d-463d-c604-e5a3ee93478f"
      },
      "source": [
        "epochs = 100\n",
        "train_loss = 0\n",
        "train_acc = 0\n",
        "valid_loss = 0\n",
        "valid_acc = 0\n",
        "macro_f1  = 0\n",
        "best_macro_f1 = float('0')\n",
        "for epoch in range(epochs):\n",
        "  start_time = time.time()\n",
        "  train_loss,train_acc = train()\n",
        "  valid_loss,valid_acc,macro_f1 = evaluate()\n",
        "    \n",
        "  end_time = time.time()\n",
        "        \n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "  if macro_f1 > best_macro_f1:\n",
        "    best_macro_f1 = macro_f1\n",
        "    torch.save(model,'model_english_task_a.pt')\n",
        "  \n",
        "  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "  #print(f'\\t macro_f1: {macro_f1:.3f} |  c: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.99      0.67      0.80      1353\n",
            "           2       0.13      0.54      0.20        26\n",
            "           3       0.40      0.79      0.53        84\n",
            "\n",
            "    accuracy                           0.68      1463\n",
            "   macro avg       0.38      0.50      0.38      1463\n",
            "weighted avg       0.94      0.68      0.78      1463\n",
            "\n",
            "accuracy = 0.68\n",
            "Epoch: 01 | Epoch Time: 2m 8s\n",
            "\tTrain Loss: 0.975 | Train Acc: 62.82%\n",
            "\t Val. Loss: 0.841 |  Val. Acc: 67.81%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.11      0.37      0.17        78\n",
            "           1       0.85      0.73      0.79      1073\n",
            "           2       0.38      0.30      0.33       142\n",
            "           3       0.56      0.54      0.55       170\n",
            "\n",
            "    accuracy                           0.65      1463\n",
            "   macro avg       0.47      0.48      0.46      1463\n",
            "weighted avg       0.73      0.65      0.68      1463\n",
            "\n",
            "accuracy = 0.65\n",
            "Epoch: 02 | Epoch Time: 2m 8s\n",
            "\tTrain Loss: 0.814 | Train Acc: 66.89%\n",
            "\t Val. Loss: 0.871 |  Val. Acc: 64.66%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.30      0.29       246\n",
            "           1       0.76      0.72      0.74       968\n",
            "           2       0.27      0.38      0.31        80\n",
            "           3       0.55      0.53      0.54       169\n",
            "\n",
            "    accuracy                           0.61      1463\n",
            "   macro avg       0.46      0.48      0.47      1463\n",
            "weighted avg       0.63      0.61      0.62      1463\n",
            "\n",
            "accuracy = 0.61\n",
            "Epoch: 03 | Epoch Time: 2m 8s\n",
            "\tTrain Loss: 0.635 | Train Acc: 75.16%\n",
            "\t Val. Loss: 0.976 |  Val. Acc: 60.97%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.29      0.25       206\n",
            "           1       0.79      0.73      0.76       995\n",
            "           2       0.23      0.37      0.29        70\n",
            "           3       0.56      0.47      0.51       192\n",
            "\n",
            "    accuracy                           0.62      1463\n",
            "   macro avg       0.45      0.47      0.45      1463\n",
            "weighted avg       0.65      0.62      0.63      1463\n",
            "\n",
            "accuracy = 0.62\n",
            "Epoch: 04 | Epoch Time: 2m 8s\n",
            "\tTrain Loss: 0.415 | Train Acc: 85.45%\n",
            "\t Val. Loss: 1.310 |  Val. Acc: 61.86%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.17      0.28      0.21       158\n",
            "           1       0.83      0.72      0.77      1076\n",
            "           2       0.26      0.38      0.31        77\n",
            "           3       0.52      0.55      0.53       152\n",
            "\n",
            "    accuracy                           0.63      1463\n",
            "   macro avg       0.44      0.48      0.46      1463\n",
            "weighted avg       0.70      0.63      0.66      1463\n",
            "\n",
            "accuracy = 0.63\n",
            "Epoch: 05 | Epoch Time: 2m 8s\n",
            "\tTrain Loss: 0.264 | Train Acc: 91.99%\n",
            "\t Val. Loss: 1.765 |  Val. Acc: 63.43%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.29      0.32       345\n",
            "           1       0.70      0.75      0.72       868\n",
            "           2       0.32      0.34      0.33       107\n",
            "           3       0.51      0.58      0.54       143\n",
            "\n",
            "    accuracy                           0.59      1463\n",
            "   macro avg       0.48      0.49      0.48      1463\n",
            "weighted avg       0.58      0.59      0.58      1463\n",
            "\n",
            "accuracy = 0.59\n",
            "Epoch: 06 | Epoch Time: 2m 7s\n",
            "\tTrain Loss: 0.173 | Train Acc: 95.10%\n",
            "\t Val. Loss: 2.260 |  Val. Acc: 59.13%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.30      0.30       266\n",
            "           1       0.77      0.74      0.75       955\n",
            "           2       0.26      0.35      0.30        84\n",
            "           3       0.50      0.52      0.51       158\n",
            "\n",
            "    accuracy                           0.61      1463\n",
            "   macro avg       0.46      0.48      0.47      1463\n",
            "weighted avg       0.62      0.61      0.62      1463\n",
            "\n",
            "accuracy = 0.61\n",
            "Epoch: 07 | Epoch Time: 2m 7s\n",
            "\tTrain Loss: 0.107 | Train Acc: 97.31%\n",
            "\t Val. Loss: 2.363 |  Val. Acc: 61.38%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.28      0.34       394\n",
            "           1       0.71      0.76      0.73       863\n",
            "           2       0.22      0.35      0.27        69\n",
            "           3       0.47      0.56      0.51       137\n",
            "\n",
            "    accuracy                           0.59      1463\n",
            "   macro avg       0.45      0.49      0.46      1463\n",
            "weighted avg       0.59      0.59      0.58      1463\n",
            "\n",
            "accuracy = 0.59\n",
            "Epoch: 08 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.078 | Train Acc: 97.95%\n",
            "\t Val. Loss: 2.660 |  Val. Acc: 59.19%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.29      0.34       359\n",
            "           1       0.71      0.76      0.73       869\n",
            "           2       0.30      0.33      0.31        99\n",
            "           3       0.47      0.57      0.52       136\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.47      0.49      0.47      1463\n",
            "weighted avg       0.58      0.60      0.59      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 09 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.058 | Train Acc: 98.50%\n",
            "\t Val. Loss: 2.745 |  Val. Acc: 59.67%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 10 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.043 | Train Acc: 98.82%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 11 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.032 | Train Acc: 99.09%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 12 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.036 | Train Acc: 99.11%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 13 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.034 | Train Acc: 98.95%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 14 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.032 | Train Acc: 99.13%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 15 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.031 | Train Acc: 99.16%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 16 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.029 | Train Acc: 99.20%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 17 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.036 | Train Acc: 98.98%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 18 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.036 | Train Acc: 99.07%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 19 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.035 | Train Acc: 99.04%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 20 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.033 | Train Acc: 99.13%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 21 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.035 | Train Acc: 99.04%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 22 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.032 | Train Acc: 99.18%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 23 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.035 | Train Acc: 99.19%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 24 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.033 | Train Acc: 99.09%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 25 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.032 | Train Acc: 99.11%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 26 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.034 | Train Acc: 99.07%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 27 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.031 | Train Acc: 99.18%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 28 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.036 | Train Acc: 99.07%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 29 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.034 | Train Acc: 99.09%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 30 | Epoch Time: 2m 7s\n",
            "\tTrain Loss: 0.034 | Train Acc: 99.04%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 31 | Epoch Time: 2m 7s\n",
            "\tTrain Loss: 0.037 | Train Acc: 99.04%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 32 | Epoch Time: 2m 7s\n",
            "\tTrain Loss: 0.030 | Train Acc: 99.20%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 33 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.034 | Train Acc: 99.18%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 34 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.035 | Train Acc: 99.04%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 35 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.036 | Train Acc: 99.00%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 36 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.037 | Train Acc: 98.95%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 37 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.035 | Train Acc: 99.04%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 38 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.036 | Train Acc: 99.13%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 39 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.034 | Train Acc: 99.09%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 40 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.031 | Train Acc: 99.11%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 41 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.036 | Train Acc: 98.95%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 42 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.031 | Train Acc: 99.23%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 43 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.034 | Train Acc: 99.09%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 44 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.034 | Train Acc: 99.11%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 45 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.035 | Train Acc: 99.07%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 46 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.032 | Train Acc: 99.13%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 47 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.037 | Train Acc: 99.00%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 48 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.034 | Train Acc: 99.13%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 49 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.035 | Train Acc: 99.02%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 50 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.034 | Train Acc: 99.16%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.29      0.30       283\n",
            "           1       0.74      0.75      0.75       914\n",
            "           2       0.28      0.32      0.30        97\n",
            "           3       0.51      0.49      0.50       169\n",
            "\n",
            "    accuracy                           0.60      1463\n",
            "   macro avg       0.46      0.46      0.46      1463\n",
            "weighted avg       0.60      0.60      0.60      1463\n",
            "\n",
            "accuracy = 0.60\n",
            "Epoch: 51 | Epoch Time: 2m 6s\n",
            "\tTrain Loss: 0.034 | Train Acc: 98.95%\n",
            "\t Val. Loss: 2.710 |  Val. Acc: 60.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZDTn7vd0K0M"
      },
      "source": [
        "del model\n",
        "import gc\n",
        "gc.collect()\n",
        "  \n",
        "model = torch.load('model_english_task_b.pt')\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhIT-nGy0M3a"
      },
      "source": [
        "def evaluate_test():\n",
        "    model.eval()\n",
        "        \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    number_of_eval_steps= 0\n",
        "    \n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "    \n",
        "    all_sentence_id=[]\n",
        "\n",
        "    for batch in test_data_loader:\n",
        "        b_sentence_id = batch[0].to(device)\n",
        "        b_input_ids = batch[1].to(device)\n",
        "        b_input_mask = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        sent_ids = b_sentence_id.to('cpu').numpy()\n",
        "        all_sentence_id.extend(sent_ids)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = model(b_input_ids,\n",
        "                                attention_mask= b_input_mask,\n",
        "                                labels = b_labels.long())\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss+=loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu()\n",
        "\n",
        "        label_ids = b_labels.to('cpu')\n",
        "        \n",
        "\n",
        "        pred,true = predictions_labels(logits,label_ids)\n",
        "        \n",
        "        all_pred_labels.extend(pred)\n",
        "        \n",
        "        all_true_labels.extend(true)\n",
        "\n",
        "    print(classification_report(all_pred_labels,all_true_labels))\n",
        "    avg_val_accuracy = accuracy_score(all_pred_labels,all_true_labels)\n",
        "    \n",
        "    avg_val_loss = total_eval_loss/len(valid_data_loader)\n",
        "\n",
        "    print(\"accuracy = {0:.2f}\".format(avg_val_accuracy))\n",
        "    \n",
        "    return avg_val_loss,avg_val_accuracy,all_sentence_id,all_pred_labels\n",
        "\n",
        "valid_loss,valid_acc,all_sentence_id,all_pred_labels = evaluate_test()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}