{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG4nzUe9Z2Ss"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO4QaVHhFQzt",
        "outputId": "b0d19006-4b56-44a6-edf1-bf4b4e7c96ed"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZDvDkctE01D",
        "outputId": "aa26c9ca-33ad-476b-bc10-6638a09af99f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn import model_selection\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from pycontractions import pycontractions\n",
        "\n",
        "\n",
        "np.random.seed(666)\n",
        "df = pd.read_csv(\"english_dataset.tsv\", sep=\"\\\\t\", engine='python')\n",
        "\n",
        "df[\"text\"].dropna(inplace = True)\n",
        "df[\"text\"] = [re.sub('\\d', \" \", i) for i in df[\"text\"]]\n",
        "df[\"text\"] = [re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", i) for i in df['text']]\n",
        "\n",
        "df[\"text\"] = [word_tokenize(i) for i in df[\"text\"]]\n",
        "\n",
        "tag_dict = defaultdict(lambda: wn.NOUN)\n",
        "tag_dict[\"J\"] = wn.ADJ\n",
        "tag_dict[\"V\"] = wn.VERB\n",
        "tag_dict[\"R\"] = wn.ADV\n",
        "\n",
        "for index, val in enumerate(tqdm(df[\"text\"])):\n",
        "    res = []\n",
        "    word_lemmatized = WordNetLemmatizer()\n",
        "    for word, tag in pos_tag(val):\n",
        "        if word not in stopwords.words(\"english\") and word.isalpha():\n",
        "            word_final = word_lemmatized.lemmatize(word, tag_dict[tag[0]])\n",
        "            res.append(word_final)\n",
        "    df.loc[index, \"text_final\"] = str(res).lower()\n",
        "\n",
        "Train_X, Test_X, Train_Y1, Test_Y1, Train_Y2, Test_Y2, Train_Y3, Test_Y3 = model_selection.train_test_split(df['text_final'],df['task_1'],df['task_2'],df['task_3'],test_size=0.3)\n",
        "\n",
        "Encoder = LabelEncoder()\n",
        "Train_Y1 = Encoder.fit_transform(Train_Y1)\n",
        "Test_Y1 = Encoder.fit_transform(Test_Y1)\n",
        "Train_Y2 = Encoder.fit_transform(Train_Y2)\n",
        "Test_Y2 = Encoder.fit_transform(Test_Y2)\n",
        "Train_Y3 = Encoder.fit_transform(Train_Y3)\n",
        "Test_Y3 = Encoder.fit_transform(Test_Y3)\n",
        "\n",
        "pickle.dump(Train_Y1, open(\"TrainY1.pkl\", \"wb\"))\n",
        "pickle.dump(Test_Y1, open(\"TestY1.pkl\", \"wb\"))\n",
        "pickle.dump(Train_Y2, open(\"TrainY2.pkl\", \"wb\"))\n",
        "pickle.dump(Test_Y2, open(\"TestY2.pkl\", \"wb\"))\n",
        "pickle.dump(Train_Y3, open(\"TrainY3.pkl\", \"wb\"))\n",
        "pickle.dump(Test_Y3, open(\"TestY3.pkl\", \"wb\"))\n",
        "\n",
        "tfidf_vector = TfidfVectorizer(max_features= 5000)\n",
        "tfidf_vector.fit(df[\"text_final\"])\n",
        "\n",
        "Train_X_Tfidf = tfidf_vector.transform(Train_X)\n",
        "Test_X_Tfidf = tfidf_vector.transform(Test_X)\n",
        "\n",
        "pickle.dump(Train_X_Tfidf, open(\"TrainXTfidf.pkl\", \"wb\"))\n",
        "pickle.dump(Test_X_Tfidf, open(\"TestXTfidf.pkl\", \"wb\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5852/5852 [00:36<00:00, 159.91it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dZrYwnSE8y4",
        "outputId": "609edb9e-a89b-4c97-f03d-708fadb7d48e"
      },
      "source": [
        "len(Train_X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4096"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouObm-qIHRN9",
        "outputId": "1151d016-4aca-4e32-c0d8-3ee839aa5d9b"
      },
      "source": [
        "len(Train_X[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX9OqXP6HXtC",
        "outputId": "b0fa79cc-eddd-4aa6-b284-953fb4f6d732"
      },
      "source": [
        "tfidf_vector.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aajtak',\n",
              " 'abandon',\n",
              " 'abc',\n",
              " 'ability',\n",
              " 'able',\n",
              " 'abortion',\n",
              " 'about',\n",
              " 'abpanandatv',\n",
              " 'abpnewstv',\n",
              " 'abrandnewdayoutnow',\n",
              " 'absence',\n",
              " 'absolute',\n",
              " 'absolutely',\n",
              " 'abt',\n",
              " 'abu',\n",
              " 'abuse',\n",
              " 'abuser',\n",
              " 'abusing',\n",
              " 'abusive',\n",
              " 'ac',\n",
              " 'accept',\n",
              " 'acceptable',\n",
              " 'access',\n",
              " 'accident',\n",
              " 'accidentally',\n",
              " 'accomplish',\n",
              " 'accord',\n",
              " 'according',\n",
              " 'account',\n",
              " 'accountability',\n",
              " 'accountable',\n",
              " 'acct',\n",
              " 'accuse',\n",
              " 'accused',\n",
              " 'achieve',\n",
              " 'achievement',\n",
              " 'acid',\n",
              " 'acosta',\n",
              " 'across',\n",
              " 'act',\n",
              " 'actbrigitte',\n",
              " 'action',\n",
              " 'activism',\n",
              " 'activist',\n",
              " 'activity',\n",
              " 'actor',\n",
              " 'actual',\n",
              " 'actually',\n",
              " 'ad',\n",
              " 'adam',\n",
              " 'adamcbest',\n",
              " 'add',\n",
              " 'additional',\n",
              " 'address',\n",
              " 'adgpi',\n",
              " 'adhir',\n",
              " 'adhirrcinc',\n",
              " 'adityarajkaul',\n",
              " 'adivce',\n",
              " 'admin',\n",
              " 'administration',\n",
              " 'admire',\n",
              " 'admit',\n",
              " 'admitted',\n",
              " 'adult',\n",
              " 'adulterer',\n",
              " 'advance',\n",
              " 'advantage',\n",
              " 'adversary',\n",
              " 'advice',\n",
              " 'advisor',\n",
              " 'advocate',\n",
              " 'af',\n",
              " 'affair',\n",
              " 'affect',\n",
              " 'afford',\n",
              " 'afg',\n",
              " 'afghanistan',\n",
              " 'afraid',\n",
              " 'africa',\n",
              " 'afridarahmanali',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'age',\n",
              " 'agency',\n",
              " 'agenda',\n",
              " 'agent',\n",
              " 'agitate',\n",
              " 'agitation',\n",
              " 'ago',\n",
              " 'agree',\n",
              " 'agreed',\n",
              " 'agreement',\n",
              " 'ah',\n",
              " 'ahead',\n",
              " 'ai',\n",
              " 'aid',\n",
              " 'aiims',\n",
              " 'aiimsrda',\n",
              " 'aimim',\n",
              " 'air',\n",
              " 'aircraft',\n",
              " 'aitcofficial',\n",
              " 'aka',\n",
              " 'akin',\n",
              " 'al',\n",
              " 'alabama',\n",
              " 'alert',\n",
              " 'ali',\n",
              " 'alien',\n",
              " 'aligarh',\n",
              " 'aligarhmurdercase',\n",
              " 'alive',\n",
              " 'alivelshi',\n",
              " 'all',\n",
              " 'allah',\n",
              " 'allege',\n",
              " 'alleged',\n",
              " 'allegedly',\n",
              " 'allotment',\n",
              " 'allow',\n",
              " 'ally',\n",
              " 'almond',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'alongside',\n",
              " 'already',\n",
              " 'alright',\n",
              " 'also',\n",
              " 'altafhussain',\n",
              " 'altercation',\n",
              " 'alternative',\n",
              " 'always',\n",
              " 'am',\n",
              " 'amash',\n",
              " 'amaze',\n",
              " 'amazing',\n",
              " 'amazingdeals',\n",
              " 'amazon',\n",
              " 'amazonin',\n",
              " 'ambassador',\n",
              " 'ambition',\n",
              " 'america',\n",
              " 'americafirst',\n",
              " 'american',\n",
              " 'americanlegion',\n",
              " 'americans',\n",
              " 'americansforimpeachment',\n",
              " 'amid',\n",
              " 'amit',\n",
              " 'amitshah',\n",
              " 'amitshahoffice',\n",
              " 'among',\n",
              " 'amount',\n",
              " 'amp',\n",
              " 'an',\n",
              " 'analysis',\n",
              " 'and',\n",
              " 'andrewyang',\n",
              " 'andylassner',\n",
              " 'angela',\n",
              " 'anger',\n",
              " 'angrily',\n",
              " 'angry',\n",
              " 'ani',\n",
              " 'animal',\n",
              " 'aninewsup',\n",
              " 'anjanaomkashyap',\n",
              " 'anniversary',\n",
              " 'announce',\n",
              " 'announcement',\n",
              " 'annoy',\n",
              " 'anoint',\n",
              " 'another',\n",
              " 'answer',\n",
              " 'ant',\n",
              " 'anthem',\n",
              " 'anthonyguidera',\n",
              " 'anti',\n",
              " 'antidote',\n",
              " 'antimuslim',\n",
              " 'antitrump',\n",
              " 'antitrumpaf',\n",
              " 'any',\n",
              " 'anybody',\n",
              " 'anymore',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anytime',\n",
              " 'anyway',\n",
              " 'anywhere',\n",
              " 'aoc',\n",
              " 'ap',\n",
              " 'apart',\n",
              " 'apologise',\n",
              " 'apologize',\n",
              " 'apology',\n",
              " 'app',\n",
              " 'appalling',\n",
              " 'apparently',\n",
              " 'appeal',\n",
              " 'appear',\n",
              " 'appearance',\n",
              " 'appease',\n",
              " 'appeasement',\n",
              " 'applaud',\n",
              " 'apple',\n",
              " 'apply',\n",
              " 'appoint',\n",
              " 'appointment',\n",
              " 'appreciate',\n",
              " 'apprehensive',\n",
              " 'approach',\n",
              " 'appropriate',\n",
              " 'approval',\n",
              " 'aprildryan',\n",
              " 'aquaman',\n",
              " 'arabia',\n",
              " 'aranganathan',\n",
              " 'arcanegoat',\n",
              " 'are',\n",
              " 'area',\n",
              " 'argue',\n",
              " 'argument',\n",
              " 'arise',\n",
              " 'arkansas',\n",
              " 'arm',\n",
              " 'armed',\n",
              " 'army',\n",
              " 'arnab',\n",
              " 'arnabgoswamirtv',\n",
              " 'arnold',\n",
              " 'around',\n",
              " 'arrange',\n",
              " 'arrangement',\n",
              " 'arrest',\n",
              " 'arresttrump',\n",
              " 'arrive',\n",
              " 'arrogance',\n",
              " 'arrogant',\n",
              " 'arse',\n",
              " 'arsed',\n",
              " 'art',\n",
              " 'article',\n",
              " 'artist',\n",
              " 'arvindkejriwal',\n",
              " 'aryanagentorange',\n",
              " 'as',\n",
              " 'asadowaisi',\n",
              " 'asap',\n",
              " 'ashamed',\n",
              " 'asian',\n",
              " 'asifzardarippp',\n",
              " 'ask',\n",
              " 'askanshul',\n",
              " 'asking',\n",
              " 'askstar',\n",
              " 'asleep',\n",
              " 'aspect',\n",
              " 'ass',\n",
              " 'assange',\n",
              " 'assassination',\n",
              " 'assault',\n",
              " 'assaulted',\n",
              " 'assaulter',\n",
              " 'assaultondoctors',\n",
              " 'assert',\n",
              " 'asshat',\n",
              " 'asshole',\n",
              " 'associate',\n",
              " 'association',\n",
              " 'assume',\n",
              " 'assurance',\n",
              " 'assure',\n",
              " 'asthma',\n",
              " 'at',\n",
              " 'athlete',\n",
              " 'atleast',\n",
              " 'atmosphere',\n",
              " 'atrocious',\n",
              " 'atrocity',\n",
              " 'atrupar',\n",
              " 'attach',\n",
              " 'attack',\n",
              " 'attacker',\n",
              " 'attacks',\n",
              " 'attempt',\n",
              " 'attend',\n",
              " 'attention',\n",
              " 'attitude',\n",
              " 'attorney',\n",
              " 'auburn',\n",
              " 'audience',\n",
              " 'aus',\n",
              " 'auschwitz',\n",
              " 'auspol',\n",
              " 'australia',\n",
              " 'australian',\n",
              " 'ausvind',\n",
              " 'ausvsind',\n",
              " 'ausvwi',\n",
              " 'authentic',\n",
              " 'author',\n",
              " 'authorities',\n",
              " 'authority',\n",
              " 'auto',\n",
              " 'autocratic',\n",
              " 'automatically',\n",
              " 'av',\n",
              " 'available',\n",
              " 'ave',\n",
              " 'average',\n",
              " 'avoid',\n",
              " 'aw',\n",
              " 'await',\n",
              " 'awaited',\n",
              " 'award',\n",
              " 'aware',\n",
              " 'away',\n",
              " 'awesome',\n",
              " 'baat',\n",
              " 'baby',\n",
              " 'back',\n",
              " 'background',\n",
              " 'backlash',\n",
              " 'bad',\n",
              " 'badge',\n",
              " 'badly',\n",
              " 'bag',\n",
              " 'bail',\n",
              " 'balance',\n",
              " 'balidaan',\n",
              " 'balidaanbadge',\n",
              " 'balidan',\n",
              " 'balidanbadge',\n",
              " 'ball',\n",
              " 'ban',\n",
              " 'band',\n",
              " 'banerjee',\n",
              " 'bang',\n",
              " 'bangal',\n",
              " 'bangalore',\n",
              " 'bangladesh',\n",
              " 'bangladeshis',\n",
              " 'bank',\n",
              " 'bankrupt',\n",
              " 'bankruptcy',\n",
              " 'banner',\n",
              " 'bannon',\n",
              " 'bano',\n",
              " 'bar',\n",
              " 'barack',\n",
              " 'barackobama',\n",
              " 'barbara',\n",
              " 'barbie',\n",
              " 'barcrawl',\n",
              " 'bare',\n",
              " 'barely',\n",
              " 'barf',\n",
              " 'barish',\n",
              " 'barr',\n",
              " 'barrage',\n",
              " 'barrcoverup',\n",
              " 'barrhearing',\n",
              " 'barrlied',\n",
              " 'bars',\n",
              " 'base',\n",
              " 'baseless',\n",
              " 'bash',\n",
              " 'basic',\n",
              " 'basically',\n",
              " 'basis',\n",
              " 'basketball',\n",
              " 'bastard',\n",
              " 'bat',\n",
              " 'batch',\n",
              " 'batting',\n",
              " 'battle',\n",
              " 'bbc',\n",
              " 'bbhuttozardari',\n",
              " 'bc',\n",
              " 'bcbtigers',\n",
              " 'bcci',\n",
              " 'bccidomestic',\n",
              " 'bcoz',\n",
              " 'bcs',\n",
              " 'bcz',\n",
              " 'bd',\n",
              " 'bday',\n",
              " 'bdutt',\n",
              " 'be',\n",
              " 'beacon',\n",
              " 'bear',\n",
              " 'beat',\n",
              " 'beautiful',\n",
              " 'beauty',\n",
              " 'bebest',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becponder',\n",
              " 'bed',\n",
              " 'beef',\n",
              " 'been',\n",
              " 'beer',\n",
              " 'beerluvr',\n",
              " 'befool',\n",
              " 'before',\n",
              " 'beg',\n",
              " 'beggar',\n",
              " 'begin',\n",
              " 'begonebigots',\n",
              " 'behalf',\n",
              " 'behave',\n",
              " 'behavior',\n",
              " 'behaviour',\n",
              " 'behind',\n",
              " 'beijing',\n",
              " 'being',\n",
              " 'belief',\n",
              " 'believe',\n",
              " 'believes',\n",
              " 'belong',\n",
              " 'belongs',\n",
              " 'beloved',\n",
              " 'belt',\n",
              " 'bend',\n",
              " 'benedict',\n",
              " 'benefit',\n",
              " 'bengal',\n",
              " 'bengalburning',\n",
              " 'bengaldoctorsstrike',\n",
              " 'bengali',\n",
              " 'bengalis',\n",
              " 'bengaluru',\n",
              " 'bengalviolence',\n",
              " 'benghazi',\n",
              " 'benmaller',\n",
              " 'bent',\n",
              " 'bernie',\n",
              " 'berniesanders',\n",
              " 'besides',\n",
              " 'best',\n",
              " 'bet',\n",
              " 'betray',\n",
              " 'betrayal',\n",
              " 'better',\n",
              " 'bettybowers',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'bhai',\n",
              " 'bhakts',\n",
              " 'bhamla',\n",
              " 'bharat',\n",
              " 'bharatroarsonday',\n",
              " 'bhi',\n",
              " 'bhogleharsha',\n",
              " 'bhorangzen',\n",
              " 'bias',\n",
              " 'bible',\n",
              " 'bid',\n",
              " 'biden',\n",
              " 'big',\n",
              " 'biggest',\n",
              " 'bigot',\n",
              " 'bihar',\n",
              " 'bike',\n",
              " 'bill',\n",
              " 'billclinton',\n",
              " 'billcosby',\n",
              " 'billion',\n",
              " 'billiondollarloser',\n",
              " 'billmaher',\n",
              " 'bin',\n",
              " 'birth',\n",
              " 'birthday',\n",
              " 'bisexual',\n",
              " 'bit',\n",
              " 'bitch',\n",
              " 'bitcoin',\n",
              " 'bite',\n",
              " 'bj',\n",
              " 'bjp',\n",
              " 'black',\n",
              " 'blacklivesmatter',\n",
              " 'blackmail',\n",
              " 'blade',\n",
              " 'blah',\n",
              " 'blame',\n",
              " 'blanket',\n",
              " 'blankie',\n",
              " 'blast',\n",
              " 'bleedblue',\n",
              " 'bless',\n",
              " 'blind',\n",
              " 'block',\n",
              " 'blockchain',\n",
              " 'blocked',\n",
              " 'blocking',\n",
              " 'blood',\n",
              " 'bloody',\n",
              " 'blotus',\n",
              " 'blow',\n",
              " 'blue',\n",
              " 'bluetooth',\n",
              " 'bluewave',\n",
              " 'board',\n",
              " 'boat',\n",
              " 'body',\n",
              " 'bojo',\n",
              " 'bold',\n",
              " 'bollockstoboris',\n",
              " 'bollockstobrexit',\n",
              " 'bomb',\n",
              " 'bone',\n",
              " 'book',\n",
              " 'boom',\n",
              " 'border',\n",
              " 'boring',\n",
              " 'boris',\n",
              " 'borisbehindbars',\n",
              " 'borisdoesnotrepresentme',\n",
              " 'borisjohnson',\n",
              " 'borisjohnsonshouldnotbepm',\n",
              " 'bos',\n",
              " 'bot',\n",
              " 'both',\n",
              " 'bother',\n",
              " 'bottle',\n",
              " 'bottom',\n",
              " 'bout',\n",
              " 'bow',\n",
              " 'bowl',\n",
              " 'box',\n",
              " 'boy',\n",
              " 'boycott',\n",
              " 'boycottbankalfalah',\n",
              " 'boycottchina',\n",
              " 'boycotticc',\n",
              " 'boycottworldcup',\n",
              " 'boywithluv',\n",
              " 'bozo',\n",
              " 'br',\n",
              " 'brain',\n",
              " 'branch',\n",
              " 'brand',\n",
              " 'brave',\n",
              " 'break',\n",
              " 'breaking',\n",
              " 'breakingnews',\n",
              " 'brexit',\n",
              " 'brexitshambles',\n",
              " 'briankarem',\n",
              " 'briankolfage',\n",
              " 'brianstelter',\n",
              " 'bribe',\n",
              " 'bridge',\n",
              " 'brief',\n",
              " 'briefing',\n",
              " 'bright',\n",
              " 'brilliant',\n",
              " 'bring',\n",
              " 'britain',\n",
              " 'british',\n",
              " 'brochurelive',\n",
              " 'brockturner',\n",
              " 'broke',\n",
              " 'broken',\n",
              " 'bronco',\n",
              " 'brother',\n",
              " 'brown',\n",
              " 'brutal',\n",
              " 'brutally',\n",
              " 'bs',\n",
              " 'bsybjp',\n",
              " 'bt',\n",
              " 'btw',\n",
              " 'bud',\n",
              " 'buddy',\n",
              " 'budget',\n",
              " 'buffoon',\n",
              " 'build',\n",
              " 'bullshit',\n",
              " 'bully',\n",
              " 'bumbling',\n",
              " 'bunch',\n",
              " 'burden',\n",
              " 'burdwan',\n",
              " 'burn',\n",
              " 'burning',\n",
              " 'bury',\n",
              " 'bus',\n",
              " 'bush',\n",
              " 'business',\n",
              " 'businessman',\n",
              " 'busy',\n",
              " 'but',\n",
              " 'butt',\n",
              " 'buttigieg',\n",
              " 'buy',\n",
              " 'bx',\n",
              " 'by',\n",
              " 'bye',\n",
              " 'byefelicia',\n",
              " 'bz',\n",
              " 'ca',\n",
              " 'cabinet',\n",
              " 'cabodude',\n",
              " 'cadet',\n",
              " 'cadetbonespurs',\n",
              " 'cage',\n",
              " 'cajunblueaz',\n",
              " 'california',\n",
              " 'call',\n",
              " 'called',\n",
              " 'calm',\n",
              " 'camera',\n",
              " 'camp',\n",
              " 'campaign',\n",
              " 'can',\n",
              " 'canada',\n",
              " 'cancel',\n",
              " 'cancer',\n",
              " 'candidate',\n",
              " 'candle',\n",
              " 'cant',\n",
              " 'cap',\n",
              " 'capability',\n",
              " 'capable',\n",
              " 'capital',\n",
              " 'caps',\n",
              " 'captain',\n",
              " 'car',\n",
              " 'card',\n",
              " 'care',\n",
              " 'career',\n",
              " 'careful',\n",
              " 'carolina',\n",
              " 'carry',\n",
              " 'cartoon',\n",
              " 'case',\n",
              " 'cash',\n",
              " 'cashback',\n",
              " 'cashcow',\n",
              " 'caste',\n",
              " 'cat',\n",
              " 'catch',\n",
              " 'cause',\n",
              " 'cb',\n",
              " 'cbi',\n",
              " 'cbsnews',\n",
              " 'cc',\n",
              " 'ccp',\n",
              " 'celebrate',\n",
              " 'celebration',\n",
              " 'celebrity',\n",
              " 'cell',\n",
              " 'census',\n",
              " 'center',\n",
              " 'central',\n",
              " 'centralpark',\n",
              " 'centre',\n",
              " 'ceo',\n",
              " 'ceremony',\n",
              " 'certain',\n",
              " 'certainly',\n",
              " 'certificate',\n",
              " 'ch',\n",
              " 'chahiye',\n",
              " 'chair',\n",
              " 'chairman',\n",
              " 'chal',\n",
              " 'challenge',\n",
              " 'chance',\n",
              " 'change',\n",
              " 'channel',\n",
              " 'chant',\n",
              " 'chaos',\n",
              " 'character',\n",
              " 'charge',\n",
              " 'charity',\n",
              " 'charlatan',\n",
              " 'charlie',\n",
              " 'chart',\n",
              " 'chase',\n",
              " 'chaser',\n",
              " 'chaudhary',\n",
              " 'cheap',\n",
              " 'cheat',\n",
              " 'cheater',\n",
              " 'check',\n",
              " 'cheer',\n",
              " 'cher',\n",
              " 'chicago',\n",
              " 'chicken',\n",
              " 'chief',\n",
              " 'child',\n",
              " 'childish',\n",
              " 'childmolester',\n",
              " 'children',\n",
              " 'chin',\n",
              " 'china',\n",
              " 'chinese',\n",
              " 'choice',\n",
              " 'chomo',\n",
              " 'choose',\n",
              " 'chris',\n",
              " 'christopher',\n",
              " 'church',\n",
              " 'cid',\n",
              " 'circus',\n",
              " 'citizen',\n",
              " 'citizens',\n",
              " 'citizenship',\n",
              " 'city',\n",
              " 'civil',\n",
              " 'claim',\n",
              " 'claims',\n",
              " 'clash',\n",
              " 'class',\n",
              " 'classic',\n",
              " 'clause',\n",
              " 'clean',\n",
              " 'clear',\n",
              " 'clearly',\n",
              " 'cleverly',\n",
              " 'click',\n",
              " 'climate',\n",
              " 'clinic',\n",
              " 'clinton',\n",
              " 'clip',\n",
              " 'close',\n",
              " 'closed',\n",
              " 'clothing',\n",
              " 'cloud',\n",
              " 'clown',\n",
              " 'club',\n",
              " 'clue',\n",
              " 'clueless',\n",
              " 'clutch',\n",
              " 'cm',\n",
              " 'cmofkarnataka',\n",
              " 'cmon',\n",
              " 'cn',\n",
              " 'cnn',\n",
              " 'cnnnews',\n",
              " 'co',\n",
              " 'coast',\n",
              " 'coat',\n",
              " 'cock',\n",
              " 'code',\n",
              " 'coffee',\n",
              " 'coin',\n",
              " 'col',\n",
              " 'cold',\n",
              " 'collapse',\n",
              " 'colleague',\n",
              " 'collect',\n",
              " 'college',\n",
              " 'collude',\n",
              " 'collusion',\n",
              " 'colonel',\n",
              " 'color',\n",
              " 'colour',\n",
              " 'coma',\n",
              " 'come',\n",
              " 'comedian',\n",
              " 'comedy',\n",
              " 'comfortable',\n",
              " 'comic',\n",
              " 'coming',\n",
              " 'commendable',\n",
              " 'comment',\n",
              " 'commentary',\n",
              " 'commentator',\n",
              " 'comments',\n",
              " 'commercial',\n",
              " 'commit',\n",
              " 'committed',\n",
              " 'committee',\n",
              " 'common',\n",
              " 'communal',\n",
              " 'communist',\n",
              " 'community',\n",
              " 'company',\n",
              " 'compare',\n",
              " 'compensation',\n",
              " 'competition',\n",
              " 'complacent',\n",
              " 'complain',\n",
              " 'complaint',\n",
              " 'complete',\n",
              " 'completely',\n",
              " 'complex',\n",
              " 'complicit',\n",
              " 'complicitgop',\n",
              " 'comply',\n",
              " 'compromise',\n",
              " 'compulsive',\n",
              " 'con',\n",
              " 'concentrate',\n",
              " 'concentration',\n",
              " 'concept',\n",
              " 'concern',\n",
              " 'concerned',\n",
              " 'conclude',\n",
              " 'conclusion',\n",
              " 'condemn',\n",
              " 'condition',\n",
              " 'conditions',\n",
              " 'condone',\n",
              " 'conduct',\n",
              " 'conference',\n",
              " 'confess',\n",
              " 'confidence',\n",
              " 'confirm',\n",
              " 'confused',\n",
              " 'cong',\n",
              " 'conggandinaaliinsult',\n",
              " 'congi',\n",
              " 'congratulations',\n",
              " 'congress',\n",
              " 'congressdoyourjob',\n",
              " 'congressman',\n",
              " 'congressmuktbharat',\n",
              " 'conman',\n",
              " 'conmen',\n",
              " 'connect',\n",
              " 'connection',\n",
              " 'consequence',\n",
              " 'conservative',\n",
              " 'conservativeleadership',\n",
              " 'conservatives',\n",
              " 'consider',\n",
              " 'conspiracy',\n",
              " 'conspire',\n",
              " 'constitution',\n",
              " 'constructive',\n",
              " 'contact',\n",
              " 'contain',\n",
              " 'contempt',\n",
              " 'contender',\n",
              " 'content',\n",
              " 'contest',\n",
              " 'context',\n",
              " 'continue',\n",
              " 'contribute',\n",
              " 'control',\n",
              " 'controversy',\n",
              " 'conversation',\n",
              " 'convict',\n",
              " 'convicted',\n",
              " 'conviction',\n",
              " 'convince',\n",
              " 'conway',\n",
              " 'cook',\n",
              " 'cool',\n",
              " 'cooperate',\n",
              " 'cop',\n",
              " 'copaamerica',\n",
              " 'cope',\n",
              " 'core',\n",
              " 'corner',\n",
              " 'correct',\n",
              " 'correction',\n",
              " 'corrupt',\n",
              " 'corruptgop',\n",
              " 'corruption',\n",
              " 'cos',\n",
              " 'cosby',\n",
              " 'cost',\n",
              " 'cough',\n",
              " 'could',\n",
              " 'council',\n",
              " 'counsel',\n",
              " 'count',\n",
              " 'countries',\n",
              " 'country',\n",
              " 'couple',\n",
              " 'course',\n",
              " 'court',\n",
              " 'courtesy',\n",
              " 'cover',\n",
              " 'coverage',\n",
              " 'covfefe',\n",
              " 'cow',\n",
              " 'coward',\n",
              " 'cowardly',\n",
              " 'coz',\n",
              " 'cr',\n",
              " 'craft',\n",
              " 'crap',\n",
              " 'crash',\n",
              " 'crazy',\n",
              " 'create',\n",
              " 'credibility',\n",
              " 'creepy',\n",
              " 'creepypence',\n",
              " 'crew',\n",
              " 'cricbuzz',\n",
              " 'cricket',\n",
              " 'cricketaakash',\n",
              " 'cricketer',\n",
              " 'cricketkacrown',\n",
              " 'cricketlive',\n",
              " 'cricketworldcup',\n",
              " 'crime',\n",
              " 'crimes',\n",
              " 'criminal',\n",
              " 'criminalinchief',\n",
              " 'cringe',\n",
              " 'crisis',\n",
              " 'critical',\n",
              " 'criticize',\n",
              " 'crook',\n",
              " 'crooked',\n",
              " 'crookedtrump',\n",
              " 'crore',\n",
              " 'cross',\n",
              " 'crotch',\n",
              " 'crowd',\n",
              " 'crown',\n",
              " 'cruel',\n",
              " 'crush',\n",
              " 'cry',\n",
              " 'csxc',\n",
              " 'cuck',\n",
              " 'cuddle',\n",
              " 'culcheth',\n",
              " 'cult',\n",
              " 'cultural',\n",
              " 'culture',\n",
              " 'cunt',\n",
              " 'cup',\n",
              " 'cure',\n",
              " 'current',\n",
              " 'currently',\n",
              " 'curse',\n",
              " 'curtain',\n",
              " 'custody',\n",
              " 'cut',\n",
              " 'cuttack',\n",
              " 'cuz',\n",
              " 'cw',\n",
              " 'cwc',\n",
              " 'cyber',\n",
              " 'dad',\n",
              " 'daddy',\n",
              " 'dagger',\n",
              " 'dahmer',\n",
              " 'daily',\n",
              " 'dalailama',\n",
              " 'damage',\n",
              " 'damaging',\n",
              " 'damn',\n",
              " 'dance',\n",
              " 'danger',\n",
              " 'dangerous',\n",
              " 'danscavino',\n",
              " 'dare',\n",
              " 'dark',\n",
              " 'dat',\n",
              " 'data',\n",
              " 'date',\n",
              " 'daughter',\n",
              " 'david',\n",
              " 'davidfrum',\n",
              " 'day',\n",
              " 'dc',\n",
              " 'de',\n",
              " 'dead',\n",
              " 'deal',\n",
              " 'dealoftheday',\n",
              " 'dean',\n",
              " 'dear',\n",
              " 'death',\n",
              " 'deathpenalty',\n",
              " 'debate',\n",
              " 'decade',\n",
              " 'decency',\n",
              " 'decent',\n",
              " 'deception',\n",
              " 'decide',\n",
              " 'decision',\n",
              " 'deck',\n",
              " 'declare',\n",
              " 'dedicate',\n",
              " 'dedication',\n",
              " 'deep',\n",
              " 'deeply',\n",
              " 'defeat',\n",
              " 'defence',\n",
              " 'defend',\n",
              " 'defense',\n",
              " 'definitely',\n",
              " 'defy',\n",
              " 'degree',\n",
              " 'delay',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hf74jzUHyKn",
        "outputId": "4aa2d7fd-1f77-4d5a-9a6b-199db6e50ba4"
      },
      "source": [
        "Train_X_Tfidf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4096x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 53300 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LZEPtBlMlqz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, sentence_size, hidden_nodes):\n",
        "        super(Net, self).__init__()\n",
        "       \n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(sentence_size, hidden_nodes)\n",
        "        self.fc2 = nn.Linear(hidden_nodes, hidden_nodes//2)\n",
        "        self.fc3 = nn.Linear(hidden_nodes//2, 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        #x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = Net(sentence_size = 5000, hidden_nodes = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBxX_lelUuIp",
        "outputId": "91c9c260-cb9a-4dd9-a5d6-587683f6cc9d"
      },
      "source": [
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=5000, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (fc3): Linear(in_features=50, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwbn6HQJWzaV",
        "outputId": "5bda3625-0937-4808-c27b-738248ab9755"
      },
      "source": [
        "type(Train_X_Tfidf.todense())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_12K7oTBYMh8",
        "outputId": "b3097feb-0b15-4f16-e746-b6a6c545e311"
      },
      "source": [
        "len(Train_Y1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4096"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npFmd44hXZwh"
      },
      "source": [
        "Train_Y1 = Train_Y1.reshape((len(Train_Y1),1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZWuSi6IVrUG",
        "outputId": "ab7f2427-3d0b-44a5-a923-8d4b4f00585b"
      },
      "source": [
        "epochs = 500\n",
        "input = Train_X_Tfidf.todense().astype(np.float32)\n",
        "target = torch.from_numpy(Train_Y1.astype(np.float32))\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer.zero_grad()\n",
        "for epoch in range(epochs):\n",
        "  output = net(torch.from_numpy(input))\n",
        "  loss = criterion(output, target)\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "  print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.4850, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4733, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4511, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4204, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3842, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3458, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3090, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2773, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2536, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2402, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2376, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2458, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2640, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2902, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3220, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3564, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3900, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4194, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4420, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4556, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4590, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4520, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4354, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4110, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3812, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3487, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3166, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2875, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2637, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2470, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2384, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2381, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2457, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2601, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2800, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3033, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3283, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3529, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3751, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3932, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4058, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4118, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4113, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4041, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3906, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3718, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3493, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3248, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3004, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2778, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2591, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2455, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2383, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2379, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2445, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2575, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2758, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2981, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3225, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3470, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3698, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3890, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4031, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4109, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4119, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4060, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3935, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3756, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3536, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3294, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3048, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2817, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2622, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2476, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2391, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2375, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2428, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2546, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2720, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2937, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3177, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3424, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3656, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3855, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4007, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4097, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4120, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4074, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3961, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3792, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3579, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3340, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3094, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2859, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2656, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2499, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2402, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2372, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2412, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2518, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2683, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2892, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3129, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3376, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3612, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3820, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3981, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4085, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4121, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4088, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3988, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3829, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3624, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3388, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3142, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2903, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2692, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2525, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2416, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2373, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2399, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2493, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2647, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2848, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3081, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3328, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3568, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3782, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3954, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4069, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4119, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4100, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4013, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3864, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3667, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3436, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3190, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2948, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2731, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2554, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2432, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2376, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2388, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2469, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2612, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2806, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3034, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3279, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3522, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3743, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3924, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4052, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4115, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4109, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4034, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3897, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3707, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3480, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3234, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2989, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2765, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2580, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2448, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2380, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2382, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2453, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2588, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2776, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3002, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3247, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3492, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3717, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3903, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4036, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4105, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4104, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4034, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3900, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3713, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3489, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3245, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3002, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2778, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2591, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2456, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2383, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2379, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2442, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2568, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2747, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2963, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3200, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3437, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3655, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3838, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3975, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4053, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4065, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4009, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3891, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3719, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3507, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3273, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3034, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2811, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2619, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2476, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2392, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2374, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2423, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2536, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2703, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2912, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3144, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3383, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3609, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3804, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3953, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4043, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4068, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4025, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3918, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3756, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3551, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3320, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3080, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2852, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2654, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2500, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2404, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2372, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2408, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2509, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2667, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2868, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3098, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3337, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3567, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3769, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3928, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4030, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4068, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4039, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3944, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3792, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3594, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3366, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3127, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2895, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2689, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2526, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2417, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2373, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2396, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2485, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2632, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2826, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3051, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3290, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3524, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3733, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3901, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4015, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4066, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4050, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3967, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3825, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3635, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3412, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3174, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2939, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2727, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2554, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2434, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2377, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2386, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2463, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2599, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2785, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3006, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3243, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3479, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3695, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3872, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3998, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4061, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4058, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3988, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3857, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3675, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3458, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3221, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2984, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2766, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2584, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2453, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2383, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2379, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2443, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2568, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2745, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2961, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3196, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3434, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3655, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3841, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3978, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4054, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4064, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4007, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3887, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3714, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3502, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3268, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3029, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2806, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2616, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2474, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2391, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2374, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2425, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2539, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2707, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2916, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3149, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3388, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3614, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3808, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3955, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4044, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4067, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4023, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3915, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3752, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3546, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3315, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3075, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2848, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2650, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2497, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2402, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2372, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2410, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2512, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2671, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2873, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3103, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3342, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3571, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3773, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3931, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4032, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4068, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4037, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3941, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3788, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3589, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3361, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3122, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2891, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2685, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2523, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2416, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2373, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2397, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2487, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2636, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2830, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3056, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3295, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3528, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3737, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3904, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4017, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4066, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4049, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3965, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3822, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3631, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3407, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3169, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2934, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2723, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2551, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2432, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2376, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2387, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2465, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2602, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2789, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3010, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3248, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3484, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3699, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3875, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3999, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4062, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4057, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3986, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3854, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3671, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3453, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3216, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2979, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2762, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2581, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2451, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2382, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2380, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2445, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2571, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2749, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2965, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3201, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3439, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3659, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3844, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3980, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4055, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4064, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4005, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3884, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3710, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3498, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3263, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3024, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2802, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2612, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2471, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2390, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2375, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2427, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2542, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2711, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2921, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3154, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3393, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3618, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3811, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3958, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4045, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4067, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4022, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3912, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3748, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3542, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3310, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3070, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2843, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2646, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2495, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2401, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2372, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2411, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2515, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2674, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2877, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3108, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3347, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3576, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3777, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3933, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4033, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4068, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4036, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3938, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3784, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3585, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3356, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3117, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2886, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2682, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2520, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2414, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2373, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2399, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2490, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2639, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2835, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3061, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3300, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3533, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3740, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3907, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4018, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4067, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4047, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3962, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3818, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3626, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3402, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3164, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2930, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2719, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2548, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2430, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2376, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2388, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2467, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2606, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU_3Va0gaFYo"
      },
      "source": [
        "temp = -1\n",
        "threshold = -1\n",
        "for i in np.arange(0, 1, 0.01):\n",
        "  temp_sum = (sum(target == (output > i)))\n",
        "  if temp <= temp_sum:\n",
        "    temp = temp_sum\n",
        "    threshold = i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZeUKD3qjZf5"
      },
      "source": [
        "y_pred = np.array(output > threshold, dtype = np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8Ry02MklICG",
        "outputId": "212ddb62-0d51-41c7-dfd4-485af8bcce4f"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       ...,\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edb2hlGRlt3r",
        "outputId": "801deec3-8f94-4048-a529-43c192bb3e8e"
      },
      "source": [
        "import sklearn\n",
        "sklearn.metrics.confusion_matrix(Train_Y1, y_pred)\n",
        "sklearn.metrics.f1_score(Train_Y1, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7601029211442409"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1hajfFma6et",
        "outputId": "bd9813b5-6803-4c0a-a390-fc12d3712469"
      },
      "source": [
        "input = Test_X_Tfidf.todense().astype(np.float32)\n",
        "Test_Y1  = Test_Y1.reshape((len(Test_Y1),1))\n",
        "target = torch.from_numpy(Test_Y1.astype(np.float32))\n",
        "output = net(torch.from_numpy(input))\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.2781, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8eK0I2ucsyG",
        "outputId": "1ca5d179-693a-4b93-ac19-2cf76730f6b8"
      },
      "source": [
        "y_pred = np.array(output > threshold, dtype = np.float32)\n",
        "sklearn.metrics.confusion_matrix(Test_Y1, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,  676],\n",
              "       [   0, 1080]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDuc2DkSc4tE",
        "outputId": "3cdf6501-17ce-44cf-aa86-c6570bc3bdd8"
      },
      "source": [
        "sklearn.metrics.f1_score(Test_Y1, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7616361071932299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwhLYFxVc6Ox"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}