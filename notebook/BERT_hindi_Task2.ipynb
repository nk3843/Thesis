{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_hindi_Task2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6Rj-7pCHz5D",
        "outputId": "6b428817-633e-481b-cdc7-f1ed481d1dd1"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DllFgq0BA3S-"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaD-nuuzA4lo",
        "outputId": "9b5bd385-26b9-40de-e4e4-9061fcc2af7a"
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "task = 'task_2'\n",
        "\n",
        "#2019 datasets \n",
        "\n",
        "df_test = pd.read_csv(\"/content/hasoc2019_hi_test_gold_2919.tsv\",sep='\\t')\n",
        "df_train = pd.read_csv(\"/content/hindi_dataset.tsv\",sep=\"\\t\")\n",
        "df_train = df_train.dropna()\n",
        "\n",
        "\n",
        "\n",
        "print(len(df_train))\n",
        "print(df_train.head())\n",
        "\n",
        "total_sentences = list(df_train['text'].values)\n",
        "total_labels = list(df_train[task].values)\n",
        "\n",
        "\n",
        "\n",
        "test_sentences = list(df_test['text'].values)\n",
        "test_labels = list(df_test[task].values)\n",
        "\n",
        "def clean_text(sentences):\n",
        "    for index,line in enumerate(sentences):\n",
        "        if \"\\n\" in line:\n",
        "            sentences[index] = line.replace(\"\\n\",\"\")\n",
        "    return sentences\n",
        "        \n",
        "total_sentences = clean_text(total_sentences)\n",
        "test_sentences = clean_text(test_sentences)\n",
        "\n",
        "def clean_labels(labels):\n",
        "    new_list= []\n",
        "    for value in labels:\n",
        "        new_list.append(value.strip())\n",
        "    return new_list\n",
        "\n",
        "total_labels = clean_labels(total_labels)\n",
        "test_labels = clean_labels(test_labels)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(total_labels)\n",
        "encoded_labels = le.transform(total_labels)\n",
        "encoded_test_labels = le.transform(test_labels)\n",
        "print(set(encoded_labels))\n",
        "\n",
        "print(len(total_sentences),len(encoded_labels),len(test_sentences),len(encoded_test_labels))\n",
        "\n",
        "print(df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4665\n",
            "         text_id  ... task_3\n",
            "0  hasoc_hi_5556  ...   NONE\n",
            "1  hasoc_hi_5648  ...    UNT\n",
            "2   hasoc_hi_164  ...    TIN\n",
            "3  hasoc_hi_3530  ...   NONE\n",
            "4  hasoc_hi_5206  ...   NONE\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "{0, 1, 2, 3}\n",
            "4665 4665 1318 1318\n",
            "            text_id  ... task_3\n",
            "0     hasoc_hi_5061  ...   NONE\n",
            "1     hasoc_hi_2090  ...    TIN\n",
            "2     hasoc_hi_2960  ...    TIN\n",
            "3      hasoc_hi_864  ...   NONE\n",
            "4       hasoc_hi_54  ...   NONE\n",
            "...             ...  ...    ...\n",
            "1313  hasoc_hi_2773  ...    UNT\n",
            "1314  hasoc_hi_7231  ...    TIN\n",
            "1315   hasoc_hi_769  ...    TIN\n",
            "1316  hasoc_hi_5725  ...    UNT\n",
            "1317  hasoc_hi_2519  ...   NONE\n",
            "\n",
            "[1318 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYZGhJ_4C-Dq",
        "outputId": "a077d53b-e44b-49f7-80b6-150bcee7bc62"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "max_length = 0\n",
        "for sentence in total_sentences:\n",
        "    #print(sentence)\n",
        "    length = len(tokenizer.tokenize(sentence))\n",
        "    if length > max_length:\n",
        "        max_length  = length\n",
        "print(\"max token length is: \",max_length)\n",
        "# max token length obtained is 50\n",
        "# bert tokens are limited to 514 bytes."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max token length is:  297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VwTBUNhEHzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a17682-5fe4-47d8-a4d2-74aeeef94364"
      },
      "source": [
        "def encoder_generator(sentences,labels):\n",
        "    \n",
        "    sent_index = []\n",
        "    input_ids = []\n",
        "    attention_masks =[]\n",
        "\n",
        "    for index,sent in enumerate(sentences):\n",
        "        \n",
        "        sent_index.append(index)\n",
        "        \n",
        "        encoded_dict = tokenizer.encode_plus(sent,\n",
        "                                             add_special_tokens=True,\n",
        "                                             max_length=128,\n",
        "                                             pad_to_max_length=True,\n",
        "                                             truncation = True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_tensors='pt')\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids,dim=0)\n",
        "    attention_masks = torch.cat(attention_masks,dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    sent_index = torch.tensor(sent_index)\n",
        "\n",
        "    return sent_index,input_ids,attention_masks,labels\n",
        "\n",
        "sent_index,input_ids,attention_masks,encoded_label_tensors = encoder_generator(total_sentences,encoded_labels)\n",
        "test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors = encoder_generator(test_sentences,encoded_test_labels)\n",
        "print('Original: ', total_sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  बांग्लादेश की शानदार वापसी, भारत को 314 रन पर रोका #INDvBAN #CWC19\n",
            "Token IDs: tensor([  101, 67260, 45470, 10826,   896, 21202, 55904, 37038, 92191, 10914,\n",
            "          117, 14311, 11267, 32031,   891, 11453, 12213,   891, 69897, 11208,\n",
            "          108, 36351, 11490, 10477, 35999, 11537,   108, 63216, 10858, 54055,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIaQBoXwEhPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d2272b-cd64-426a-9abd-5fd7bc0cf569"
      },
      "source": [
        "from torch.utils.data import TensorDataset,random_split\n",
        "\n",
        "dataset = TensorDataset(input_ids,attention_masks,encoded_label_tensors)\n",
        "test_dataset = TensorDataset(test_sent_index,test_input_ids,test_attention_masks,encoded_test_label_tensors)\n",
        "\n",
        "train_size = int(0.75*len(dataset))\n",
        "\n",
        "val_size = len(dataset)-train_size\n",
        "\n",
        "train_dataset,val_dataset = random_split(dataset,[train_size,val_size])\n",
        "\n",
        "print('train data samples is {}'.format(len(train_dataset)))\n",
        "print(\"valid data samples is {}\".format(len(val_dataset)))\n",
        "print(\"test data samples is {}\".format(len(test_dataset)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data samples is 3498\n",
            "valid data samples is 1167\n",
            "test data samples is 1318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8FTVcl6Eowm"
      },
      "source": [
        "from torch.utils.data import DataLoader,RandomSampler,SequentialSampler\n",
        "\n",
        "bs=8\n",
        "\n",
        "train_data_loader = DataLoader(train_dataset,\n",
        "                              sampler=RandomSampler(train_dataset),\n",
        "                              batch_size=bs)\n",
        "valid_data_loader = DataLoader(val_dataset,\n",
        "                              sampler=SequentialSampler(val_dataset),\n",
        "                              batch_size=bs)\n",
        "test_data_loader = DataLoader(test_dataset,\n",
        "                            sampler=SequentialSampler(test_dataset),\n",
        "                            batch_size=bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CseI8GoXEwkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6f0991-3f5d-4487-b2c8-ff59126d0a09"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased',\n",
        "                                                     num_labels=len(le.classes_),\n",
        "                                                     output_attentions=False,\n",
        "                                                     output_hidden_states=False,\n",
        "                                                     )\n",
        "#model.cpu()\n",
        "device = \"cuda:0\"\n",
        "model = model.to(device)\n",
        "model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIqD4pu5lsAf"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-8)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs=10\n",
        "total_steps = len(train_data_loader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                           num_warmup_steps=0,\n",
        "                                           num_training_steps=total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nREL2AQFNZt"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def predictions_labels(preds,labels):\n",
        "    #print(preds.device,labels.device)\n",
        "    pred = torch.argmax(preds,axis=1).flatten()\n",
        "    label = labels.flatten()\n",
        "    return pred,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HPQl9hWFSd3"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import classification_report,accuracy_score,f1_score\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDFflMIcFVs9"
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    #print(correct.device)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]]).to(device)\n",
        "\n",
        "def predictions_labels(preds,labels):\n",
        "    #print(preds.device,labels.device)\n",
        "    pred = torch.argmax(preds,axis=1).flatten()\n",
        "    label = labels.flatten()\n",
        "    return pred,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvi36_J7FZy9"
      },
      "source": [
        "def train():\n",
        "  total_train_loss = 0\n",
        "  total_train_acc = 0\n",
        "    \n",
        "  model.train() # set model in train mode for batchnorm and dropout layers in bert model\n",
        "    \n",
        "  for step,batch in enumerate(train_data_loader):\n",
        "    #print(\"**************************************************************************\")\n",
        "    #print(\"Step : \",step,\"  batch\",len(batch))\n",
        "    #print(\"**************************************************************************\")\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    model.zero_grad()\n",
        "    #loss,logits = model(b_input_ids,attention_mask=b_input_mask,labels=b_labels.long())\n",
        "    outputs = model(b_input_ids,attention_mask=b_input_mask,labels=b_labels.long())\n",
        "    loss = outputs.loss\n",
        "    logits = outputs.logits\n",
        "    #total_train_loss+=loss.detach().numpy()\n",
        "    total_train_loss+=loss.detach()\n",
        "    total_train_acc+=categorical_accuracy(logits,b_labels).item()\n",
        "            \n",
        "    loss.backward()\n",
        "            \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "            \n",
        "    optimizer.step()\n",
        "            \n",
        "    scheduler.step() #go ahead and update the learning rate\n",
        "    #print(total_train_loss,total_train_acc)\n",
        "            \n",
        "  avg_train_loss = total_train_loss/len(train_data_loader)\n",
        "  avg_train_acc = total_train_acc/len(train_data_loader)\n",
        "    \n",
        "  return avg_train_loss,avg_train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS3kPLQMFgOG"
      },
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "        \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    number_of_eval_steps= 0\n",
        "    \n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "\n",
        "    for batch in valid_data_loader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "        #loss, logits = model(b_input_ids,attention_mask= b_input_mask,labels = b_labels.long())\n",
        "          outputs = model(b_input_ids,attention_mask=b_input_mask,labels=b_labels.long())\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        #total_eval_loss+=loss.detach().numpy()\n",
        "\n",
        "        #logits = logits.detach().cpu().numpy()\n",
        "        #label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_loss+=loss.detach()        \n",
        "        logits = logits.detach()\n",
        "        label_ids = b_labels.to(device)\n",
        "\n",
        "        pred,true = predictions_labels(logits,label_ids)\n",
        "        \n",
        "        all_pred_labels.extend(pred.detach().cpu().numpy())\n",
        "        all_true_labels.extend(true.detach().cpu().numpy())\n",
        "    \n",
        "    #print(np.shape(np.array(all_pred_labels).reshape(-1,1)),np.shape(np.array(all_true_labels).reshape(-1,1)))\n",
        "\n",
        "    print(classification_report(all_pred_labels,all_true_labels))\n",
        "    avg_val_accuracy = accuracy_score(all_pred_labels,all_true_labels)\n",
        "    macro_f1_score = f1_score(all_pred_labels,all_true_labels,average='macro')\n",
        "    \n",
        "    avg_val_loss = total_eval_loss/len(valid_data_loader)\n",
        "\n",
        "    print(\"accuracy = {0:.2f}\".format(avg_val_accuracy))\n",
        "    \n",
        "    return avg_val_loss,avg_val_accuracy,macro_f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLQ5ATlIFoMz"
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmNn6N7wFtj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc1a973-6ac1-4d31-89e7-bc73d34f8b64"
      },
      "source": [
        "epochs = 10\n",
        "train_loss = 0\n",
        "train_acc = 0\n",
        "valid_loss = 0\n",
        "valid_acc = 0\n",
        "macro_f1  = 0\n",
        "best_macro_f1 = float('0')\n",
        "for epoch in range(epochs):\n",
        "  start_time = time.time()\n",
        "  train_loss,train_acc = train()\n",
        "  valid_loss,valid_acc,macro_f1 = evaluate()\n",
        "    \n",
        "  end_time = time.time()\n",
        "        \n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "  if macro_f1 > best_macro_f1:\n",
        "    best_macro_f1 = macro_f1\n",
        "    torch.save(model,'model_hindi_task_b.pt')\n",
        "  \n",
        "  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "  #print(f'\\t macro_f1: {macro_f1:.3f} |  c: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.87      0.75      0.81       658\n",
            "           2       0.34      0.28      0.30       181\n",
            "           3       0.83      0.75      0.79       328\n",
            "\n",
            "    accuracy                           0.68      1167\n",
            "   macro avg       0.51      0.44      0.47      1167\n",
            "weighted avg       0.77      0.68      0.72      1167\n",
            "\n",
            "accuracy = 0.68\n",
            "Epoch: 01 | Epoch Time: 2m 0s\n",
            "\tTrain Loss: 1.026 | Train Acc: 60.79%\n",
            "\t Val. Loss: 0.904 |  Val. Acc: 67.69%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.03      0.36      0.06        14\n",
            "           1       0.94      0.72      0.82       739\n",
            "           2       0.35      0.41      0.38       126\n",
            "           3       0.81      0.84      0.82       288\n",
            "\n",
            "    accuracy                           0.71      1167\n",
            "   macro avg       0.53      0.58      0.52      1167\n",
            "weighted avg       0.83      0.71      0.76      1167\n",
            "\n",
            "accuracy = 0.71\n",
            "Epoch: 02 | Epoch Time: 1m 59s\n",
            "\tTrain Loss: 0.794 | Train Acc: 71.12%\n",
            "\t Val. Loss: 0.797 |  Val. Acc: 71.38%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.31      0.31       150\n",
            "           1       0.83      0.79      0.81       598\n",
            "           2       0.41      0.34      0.38       177\n",
            "           3       0.74      0.92      0.82       242\n",
            "\n",
            "    accuracy                           0.69      1167\n",
            "   macro avg       0.57      0.59      0.58      1167\n",
            "weighted avg       0.68      0.69      0.68      1167\n",
            "\n",
            "accuracy = 0.69\n",
            "Epoch: 03 | Epoch Time: 1m 59s\n",
            "\tTrain Loss: 0.693 | Train Acc: 74.17%\n",
            "\t Val. Loss: 0.816 |  Val. Acc: 68.55%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.08      0.27      0.12        45\n",
            "           1       0.82      0.81      0.81       570\n",
            "           2       0.53      0.32      0.39       247\n",
            "           3       0.87      0.85      0.86       305\n",
            "\n",
            "    accuracy                           0.70      1167\n",
            "   macro avg       0.57      0.56      0.55      1167\n",
            "weighted avg       0.74      0.70      0.71      1167\n",
            "\n",
            "accuracy = 0.70\n",
            "Epoch: 04 | Epoch Time: 1m 59s\n",
            "\tTrain Loss: 0.563 | Train Acc: 78.80%\n",
            "\t Val. Loss: 0.906 |  Val. Acc: 69.58%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.28      0.27       141\n",
            "           1       0.81      0.78      0.80       586\n",
            "           2       0.32      0.35      0.33       139\n",
            "           3       0.84      0.83      0.84       301\n",
            "\n",
            "    accuracy                           0.68      1167\n",
            "   macro avg       0.56      0.56      0.56      1167\n",
            "weighted avg       0.69      0.68      0.69      1167\n",
            "\n",
            "accuracy = 0.68\n",
            "Epoch: 05 | Epoch Time: 1m 59s\n",
            "\tTrain Loss: 0.442 | Train Acc: 83.76%\n",
            "\t Val. Loss: 1.107 |  Val. Acc: 68.29%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.30      0.19        73\n",
            "           1       0.89      0.75      0.81       674\n",
            "           2       0.35      0.37      0.36       140\n",
            "           3       0.80      0.85      0.83       280\n",
            "\n",
            "    accuracy                           0.70      1167\n",
            "   macro avg       0.55      0.57      0.55      1167\n",
            "weighted avg       0.76      0.70      0.72      1167\n",
            "\n",
            "accuracy = 0.70\n",
            "Epoch: 06 | Epoch Time: 1m 59s\n",
            "\tTrain Loss: 0.356 | Train Acc: 87.67%\n",
            "\t Val. Loss: 1.319 |  Val. Acc: 70.18%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.28      0.25       116\n",
            "           1       0.85      0.76      0.81       633\n",
            "           2       0.29      0.32      0.30       135\n",
            "           3       0.80      0.84      0.82       283\n",
            "\n",
            "    accuracy                           0.68      1167\n",
            "   macro avg       0.54      0.55      0.54      1167\n",
            "weighted avg       0.71      0.68      0.69      1167\n",
            "\n",
            "accuracy = 0.68\n",
            "Epoch: 07 | Epoch Time: 1m 59s\n",
            "\tTrain Loss: 0.281 | Train Acc: 91.24%\n",
            "\t Val. Loss: 1.451 |  Val. Acc: 68.29%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.16      0.32      0.22        79\n",
            "           1       0.86      0.76      0.81       643\n",
            "           2       0.34      0.33      0.34       155\n",
            "           3       0.81      0.83      0.82       290\n",
            "\n",
            "    accuracy                           0.69      1167\n",
            "   macro avg       0.54      0.56      0.54      1167\n",
            "weighted avg       0.73      0.69      0.71      1167\n",
            "\n",
            "accuracy = 0.69\n",
            "Epoch: 08 | Epoch Time: 1m 58s\n",
            "\tTrain Loss: 0.222 | Train Acc: 93.69%\n",
            "\t Val. Loss: 1.706 |  Val. Acc: 69.07%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.31      0.27       120\n",
            "           1       0.83      0.78      0.80       605\n",
            "           2       0.34      0.32      0.33       157\n",
            "           3       0.80      0.84      0.82       285\n",
            "\n",
            "    accuracy                           0.68      1167\n",
            "   macro avg       0.55      0.56      0.55      1167\n",
            "weighted avg       0.69      0.68      0.69      1167\n",
            "\n",
            "accuracy = 0.68\n",
            "Epoch: 09 | Epoch Time: 1m 58s\n",
            "\tTrain Loss: 0.168 | Train Acc: 95.01%\n",
            "\t Val. Loss: 1.834 |  Val. Acc: 68.12%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.32      0.27       113\n",
            "           1       0.83      0.77      0.80       615\n",
            "           2       0.34      0.34      0.34       149\n",
            "           3       0.81      0.83      0.82       290\n",
            "\n",
            "    accuracy                           0.68      1167\n",
            "   macro avg       0.55      0.56      0.56      1167\n",
            "weighted avg       0.70      0.68      0.69      1167\n",
            "\n",
            "accuracy = 0.68\n",
            "Epoch: 10 | Epoch Time: 1m 58s\n",
            "\tTrain Loss: 0.131 | Train Acc: 96.58%\n",
            "\t Val. Loss: 1.851 |  Val. Acc: 68.47%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZDTn7vd0K0M"
      },
      "source": [
        "del model\n",
        "import gc\n",
        "gc.collect()\n",
        "  \n",
        "model = torch.load('model_hindi_task_b.pt')\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhIT-nGy0M3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0623b6-6f7a-4248-e99f-891ae7168eee"
      },
      "source": [
        "def evaluate_test():\n",
        "    model.eval()\n",
        "        \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    number_of_eval_steps= 0\n",
        "    \n",
        "    all_true_labels = []\n",
        "    all_pred_labels = []\n",
        "    \n",
        "    all_sentence_id=[]\n",
        "\n",
        "    for batch in test_data_loader:\n",
        "        b_sentence_id = batch[0].to(device)\n",
        "        b_input_ids = batch[1].to(device)\n",
        "        b_input_mask = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        sent_ids = b_sentence_id.to('cpu').numpy()\n",
        "        all_sentence_id.extend(sent_ids)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = model(b_input_ids,\n",
        "                                attention_mask= b_input_mask,\n",
        "                                labels = b_labels.long())\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        total_eval_loss+=loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu()\n",
        "\n",
        "        label_ids = b_labels.to('cpu')\n",
        "        \n",
        "\n",
        "        pred,true = predictions_labels(logits,label_ids)\n",
        "        \n",
        "        all_pred_labels.extend(pred)\n",
        "        \n",
        "        all_true_labels.extend(true)\n",
        "\n",
        "    print(classification_report(all_pred_labels,all_true_labels))\n",
        "    avg_val_accuracy = accuracy_score(all_pred_labels,all_true_labels)\n",
        "    \n",
        "    avg_val_loss = total_eval_loss/len(valid_data_loader)\n",
        "\n",
        "    print(\"accuracy = {0:.2f}\".format(avg_val_accuracy))\n",
        "    \n",
        "    return avg_val_loss,avg_val_accuracy,all_sentence_id,all_pred_labels\n",
        "\n",
        "valid_loss,valid_acc,all_sentence_id,all_pred_labels = evaluate_test()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.29      0.33       245\n",
            "           1       0.77      0.85      0.81       646\n",
            "           2       0.44      0.42      0.43       209\n",
            "           3       0.83      0.83      0.83       218\n",
            "\n",
            "    accuracy                           0.68      1318\n",
            "   macro avg       0.60      0.60      0.60      1318\n",
            "weighted avg       0.66      0.68      0.66      1318\n",
            "\n",
            "accuracy = 0.68\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}